---
CONST:
  efficiency_baseline: tf-cuda-gp100gl

  gpu_flops:
    # nvidia
    gt650m: 605.77
    gtx780: 3819.42
    gtx1050: 1733.15
    gtx1070: 7282.69
    gtx1080: 9380.39
    gtx1080ti: 12571.25
    gp100gl: 10736.02
    gv100gl: 14757.70
    # amd
    r560: 1815.01
    rx480: 5950.39
    r9nano: 8077.04
    vega: 12697.10
    gfx900: 11300.84
    gfx803: 3573.79
    gfx906: 13379.70
    vega56: 4342.63
    # mali
    t628: 34.05
    # intel
    hd4000: 247.14
    hd505: 213.80
    hd630: 417.22
    uhd630: 454.72
    iris655: 757.84
    neo: 1084.50

PLATFORMS:
  pml-llvm-cpu:
    variant: linux_x86_64
  tf-cuda-gp100gl:
    variant: linux_x86_64

VARIANTS:
  linux_x86_64:
    arch: manylinux1_x86_64
    build_root: build-x86_64
    build_type: Release
    check: smoke
    system: Linux
    env:
      PLAIDML_DEVICE: llvm_cpu.0
      PLAIDML_TARGET: llvm_cpu
  linux_x86_64_dbg:
    arch: manylinux1_x86_64
    build_root: build-x86_64
    build_type: Debug
    check: core
    system: Linux
    env:
      PLAIDML_DEVICE: llvm_cpu.0
      PLAIDML_TARGET: llvm_cpu

SUITES:
  smoke:
    platforms:
      pml-llvm-cpu:
        pipelines: [nightly, plaidml]
    params:
      plaidml:
        batch_sizes: [1]
      nightly:
        batch_sizes: [1]
    wheels:
      # NOTE: this list must be in least to most dependent order.
      - plaidml-{version}-py3-none-{arch}.whl
      - plaidml_keras-{version}-py3-none-any.whl
      - plaidbench-{version}-py3-none-any.whl
    timeout: 5
    conda_env: ci/conda/tensorflow.yml
    compare: no
    eventlog: yes
    runner: plaidbench
    args:
      - --results={results}
      - --no-kernel-timing
      - --print-stacktraces
      - --no-warmup
      - --examples=1
      - keras
      - "{workload}"
    workloads:
      resnet50:
        precision: untested

  base:
    platforms:
      pml-llvm-cpu:
        pipelines: [nightly, plaidml]
    params:
      plaidml:
        batch_sizes: [0]
      nightly:
        batch_sizes: [0]
    wheels:
      # NOTE: this list must be in least to most dependent order.
      - plaidml-{version}-py3-none-{arch}.whl
      - plaidml_keras-{version}-py3-none-any.whl
    timeout: 30
    conda_env: ci/conda/tensorflow.yml
    compare: no
    runner: python
    workloads:
      backend:
        args: [backend_test.py]
        cwd: plaidml/bridge/keras
        conda_env: ci/conda/keras_backend_test.yml
        precision: untested
      # mnist_mlp:
      #   args: [mnist_mlp_test.py]
      #   precision: untested
      # regression:
      #   args: [regression_test.py]
      #   precision: untested
      trivial_model:
        args: [trivial_model_test.py]
        cwd: plaidml/bridge/keras
        precision: untested

  infer:
    platforms:
      pml-llvm-cpu:
        pipelines: [nightly, plaidml]
    params:
      plaidml:
        batch_sizes: [1]
      nightly:
        batch_sizes: [1]
    wheels:
      # NOTE: this list must be in least to most dependent order.
      - plaidml-{version}-py3-none-{arch}.whl
      - plaidml_keras-{version}-py3-none-any.whl
      - plaidbench-{version}-py3-none-any.whl
    timeout: 10
    conda_env: ci/conda/tensorflow.yml
    runner: plaidbench
    args:
      - --results={results}
      - --batch-size={batch_size}
      - --no-kernel-timing
      - --print-stacktraces
      - keras
      - "{workload}"
    precision: high
    workloads:
      inception_v3:
        platform_overrides:
          pml-llvm-cpu:
            prepend_args: [--examples=64]
      mobilenet:
      mobilenet_v2:
      resnet50:
        platform_overrides:
          pml-llvm-cpu:
            prepend_args: [--examples=128]
      vgg19:
        platform_overrides:
          pml-llvm-cpu:
            prepend_args: [--examples=64]
      xception:
        platform_overrides:
          pml-llvm-cpu:
            prepend_args: [--examples=64]
